{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77795b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "seed = 1378\n",
    "np.random.seed(seed)\n",
    "\n",
    "# فرض می‌کنیم شما یک تصویر چهره 256x256 و نقاط مربوط به آن در همان مقیاس دارید.\n",
    "def gaussian_probability(distance, sigma=15):\n",
    "        return np.exp(-(distance ** 2) / (2 * sigma ** 2))\n",
    "def process_face_image(face_img, face_landmarks):\n",
    "    # Randomly selecting a 40x40 patch in the 256x256 face image\n",
    "    x = np.random.randint(0, 256 - 40)\n",
    "    y = np.random.randint(0, 256 - 40)\n",
    "    patch = face_img[y:y + 40, x:x + 40]\n",
    "\n",
    "    # Calculating the center of the patch\n",
    "    patch_center = np.array([x + 20, y + 20])\n",
    "\n",
    "    # Defining a Gaussian function to calculate probability based on distance\n",
    "\n",
    "\n",
    "    # Calculate the probability for each face landmark based on its distance from patch center\n",
    "    probabilities = np.zeros(len(face_landmarks))\n",
    "    for i, point in enumerate(face_landmarks):\n",
    "        distance = np.linalg.norm(np.array(point) - patch_center)\n",
    "        probabilities[i] = gaussian_probability(distance)\n",
    "\n",
    "    # Normalizing the probabilities to sum to 1\n",
    "    probabilities /= np.sum(probabilities)\n",
    "\n",
    "    return probabilities , patch\n",
    "\n",
    "# فرض کنید 'face_img' و 'face_landmarks' تصویر چهره‌ی 256x256 و نقاط چهره‌ی اسکیل‌شده را دارند.\n",
    "# برای تست، می‌توانیم از تصویر و نقاط نمونه استفاده کنیم.\n",
    "# مثلاً یک تصویر سفید و نقاط تصادفی برای چهره\n",
    "\n",
    "# ایجاد یک تصویر سفید برای تست\n",
    "# face_img = cv2.imread('/media/hassan-hossein/B660FEE360FEA8EF/casia-ds/preposess/casia_test_live_10_HR_1/cropped_0025.jpg')\n",
    "\n",
    "\n",
    "# # ایجاد نقاط چهره‌ی تصادفی (در اندازه‌ی 256x256) برای تست\n",
    "# face_landmarks = np.load('/media/hassan-hossein/B660FEE360FEA8EF/casia-ds/preposess/casia_test_live_10_HR_1/cropped_0025.npy',allow_pickle=True).item()['landmark']\n",
    "\n",
    "\n",
    "# # فراخوانی تابع برای پردازش\n",
    "# P = process_face_image(face_img, face_landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78ad187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 20:46:05.986835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:05.992257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:05.993107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:05.994278: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-09 20:46:05.995152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:05.996038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:05.996894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:06.367218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:06.367776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:06.368296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-09 20:46:06.368791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22155 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "kernel_size = 20\n",
    "\n",
    "\n",
    "\n",
    "model = DenseNet121(input_shape=(kernel_size*2,kernel_size*2,3),weights=None, include_top=False)\n",
    "\n",
    "# Choose the desired low-level feature layers\n",
    "low_level_layers = [\n",
    "    model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "    model.get_layer('conv3_block12_concat')# Output of the last layer in the second dense block\n",
    "]\n",
    "# low_level_layers = [\n",
    "#     model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "#     model.get_layer('conv3_block12_concat'), \n",
    "#     model.get_layer('conv4_block24_concat')# Output of the last layer in the second dense block\n",
    "# ]\n",
    "\n",
    "# Create a new model with the desired low-level feature layers as outputs\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=[layer.output for layer in low_level_layers])\n",
    "feature_extractor.load_weights(\"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.h5\")\n",
    "feature_extractor.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078ec6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input_image = tf.random.uniform((10,40,40,3))\n",
    "def DenseNet_feature_extractor(input_image) :\n",
    "    #input_image = tf.keras.preprocessing.image.img_to_array(input_image)\n",
    "    #print(\"input shape\", input_image.shape)\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    input_image = tf.keras.applications.densenet.preprocess_input(input_image)\n",
    "    #input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    feature_extracted = feature_extractor.predict(input_image,verbose=False)\n",
    "    #print(feature_extracted[0].shape,feature_extracted[1].shape)\n",
    "    global_pooling = tf.keras.layers.GlobalAveragePooling2D()(feature_extracted[0])\n",
    "    flatten = tf.keras.layers.Flatten()(global_pooling)\n",
    "    total_feature =flatten\n",
    "    for layer_cnt in range(1,2) : \n",
    "        global_pooling = tf.keras.layers.GlobalAveragePooling2D()(feature_extracted[layer_cnt])\n",
    "        flatten = tf.keras.layers.Flatten()(global_pooling)\n",
    "        total_feature = tf.concat([flatten,total_feature],1)\n",
    "    return total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2d235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSU_path = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/MSU_MSFD_ds/MSU_MFSD/preposess/*/cropped*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0bfc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing faces:   0%|                                                                                            | 0/15549 [00:00<?, ?it/s]2024-09-09 20:46:08.626285: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2024-09-09 20:46:09.148018: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-09 20:46:09.148464: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-09 20:46:09.148477: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-09-09 20:46:09.149050: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-09 20:46:09.149092: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "Processing faces: 100%|████████████████████████████████████████████████████████████████████████████████| 15549/15549 [08:11<00:00, 31.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for graph_path in tqdm(MSU_path, desc=\"Processing faces\"):\n",
    "    # face_path = os.path.dirname(graph_path) + \"/\" + \"org_\" + os.path.basename(graph_path[:-6]).split(\"_\")[1] + \"jpg\"\n",
    "    landmark_path = graph_path[:-7] + \".npy\"\n",
    "    \n",
    "    # Load the face image\n",
    "    face = cv2.imread(graph_path[:-3])\n",
    "    \n",
    "    # Load the landmarks\n",
    "    landmark = np.load(landmark_path, allow_pickle=True).item()['landmark']\n",
    "    \n",
    "    # Process the face image and landmark to get probability and face patch\n",
    "    prob, face_patch = process_face_image(face, landmark)\n",
    "    \n",
    "    # Convert probability to tensor    \n",
    "    # Convert face patch to RGB and expand dimensions\n",
    "    face_patch = cv2.cvtColor(face_patch, cv2.COLOR_BGR2RGB)\n",
    "    face_patch = np.expand_dims(face_patch, axis=0)\n",
    "    \n",
    "    # Extract features using DenseNet\n",
    "    face_patch_feature = DenseNet_feature_extractor(face_patch).numpy()\n",
    "    #print(os.path.dirname(graph_path[:-7])+\"/\"+\"patch_feature_\"+os.path.basename(graph_path[:-7]) + \".npy\")\n",
    "    np.save(os.path.dirname(graph_path[:-7])+\"/\"+\"patch_feature_1_\"+os.path.basename(graph_path[:-7]) + \".npy\",\n",
    "           face_patch_feature)\n",
    "    np.save(os.path.dirname(graph_path[:-7])+\"/\"+\"Prob_1_\"+os.path.basename(graph_path[:-7]) + \".npy\",prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95abbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSU_path = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/Replay_attack_dataset/preposess/*/cropped*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f98e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing faces: 100%|████████████████████████████████████████████████████████████████████████████████| 61989/61989 [35:25<00:00, 29.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for graph_path in tqdm(MSU_path, desc=\"Processing faces\"):\n",
    "    # face_path = os.path.dirname(graph_path) + \"/\" + \"org_\" + os.path.basename(graph_path[:-6]).split(\"_\")[1] + \"jpg\"\n",
    "    landmark_path = graph_path[:-7] + \".npy\"\n",
    "    \n",
    "    # Load the face image\n",
    "    face = cv2.imread(graph_path[:-3])\n",
    "    \n",
    "    # Load the landmarks\n",
    "    landmark = np.load(landmark_path, allow_pickle=True).item()['landmark']\n",
    "    \n",
    "    # Process the face image and landmark to get probability and face patch\n",
    "    prob, face_patch = process_face_image(face, landmark)\n",
    "    \n",
    "    # Convert probability to tensor    \n",
    "    # Convert face patch to RGB and expand dimensions\n",
    "    face_patch = cv2.cvtColor(face_patch, cv2.COLOR_BGR2RGB)\n",
    "    face_patch = np.expand_dims(face_patch, axis=0)\n",
    "    \n",
    "    # Extract features using DenseNet\n",
    "    face_patch_feature = DenseNet_feature_extractor(face_patch).numpy()\n",
    "    #print(os.path.dirname(graph_path[:-7])+\"/\"+\"patch_feature_\"+os.path.basename(graph_path[:-7]) + \".npy\")\n",
    "    np.save(os.path.dirname(graph_path[:-7])+\"/\"+\"patch_feature_1_\"+os.path.basename(graph_path[:-7]) + \".npy\",\n",
    "           face_patch_feature)\n",
    "    np.save(os.path.dirname(graph_path[:-7])+\"/\"+\"Prob_1_\"+os.path.basename(graph_path[:-7]) + \".npy\",prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4abf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Wav2Lip)",
   "language": "python",
   "name": "wav2lip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
