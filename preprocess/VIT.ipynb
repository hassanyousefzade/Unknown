{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "047a0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch_geometric\n",
    "from PIL import Image\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import cv2\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import glob\n",
    "import os\n",
    "import math \n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import seaborn as sb\n",
    "from tqdm import tqdm\n",
    "device = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55695d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from torch import nn\n",
    "\n",
    "# بارگذاری مدل ViT با پیکربندی پایه\n",
    "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", config=config)\n",
    "\n",
    "# استفاده از تنها سه لایه اول\n",
    "class ViTThreeLayer(nn.Module):\n",
    "    def __init__(self, vit_model):\n",
    "        super(ViTThreeLayer, self).__init__()\n",
    "        # ذخیره تنها سه لایه اول\n",
    "        self.layer1 = vit_model.encoder.layer[:3]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # عبور از لایه ورودی (مانند کد نویسی ها و توکن بندی)\n",
    "        outputs = model.embeddings(x)\n",
    "        \n",
    "        # عبور از سه لایه اول\n",
    "        for layer in self.layer1:\n",
    "            outputs = layer(outputs)[0]  # گرفتن تنها آخرین وضعیت مخفی\n",
    "        \n",
    "        # استخراج ویژگی‌ها برای توکن [CLS] و فلت کردن آن به یک بردار یک‌بعدی\n",
    "        cls_token = outputs[:, 0, :]\n",
    "        return cls_token.view(-1)\n",
    "\n",
    "# نمونه‌سازی از مدل با سه لایه\n",
    "vit_three_layer = ViTThreeLayer(model)\n",
    "\n",
    "# ورودی تصادفی برای تست (1 تصویر با اندازه 224x224 و 3 کانال)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# اجرای مدل و دریافت خروجی\n",
    "with torch.no_grad():\n",
    "    output = vit_three_layer(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a4ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size= 20\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "connections = mp_face_mesh.FACEMESH_TESSELATION\n",
    "edges = []\n",
    "for connection in connections:\n",
    "    edge = [connection[0], connection[1]]\n",
    "    edges.append(edge)\n",
    "    edge  =[connection[1], connection[0]]\n",
    "    edges.append(edge)\n",
    "edges = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da03acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/oulu/preposess/*/cropped*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2708ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(paths, desc=\"Processing images\"):        \n",
    "        NAME_LIST = path.split(\"/\")[-2].split(\"_\")\n",
    "        \n",
    "        #print(face_path,NAME_LIST)\n",
    "        \n",
    "        if \"live\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"real\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"spoof\" in NAME_LIST :\n",
    "            label = 0\n",
    "        elif \"attack\" in NAME_LIST :\n",
    "            label = 0        \n",
    "        else :\n",
    "            raise 0\n",
    "            print(NAME_LIST)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #print(img.shape)\n",
    "\n",
    "        landmark_path = os.path.dirname(path) + \"/\"+os.path.basename(path).split(\".\")[0]+\".npy\"\n",
    "\n",
    "        list_of_points_face = np.load(landmark_path,allow_pickle=True).item()['landmark']\n",
    "        #print(list_of_points_face)\n",
    "\n",
    "        list_of_points_face = np.clip(list_of_points_face,0,256)\n",
    "\n",
    "        list_of_points_face = list_of_points_face + np.array([kernel_size,kernel_size])\n",
    "\n",
    "        img = np.pad(img, ((kernel_size, kernel_size), (kernel_size, kernel_size), (0, 0)),\n",
    "                 mode='constant', constant_values=0) \n",
    "\n",
    "\n",
    "        patches= []\n",
    "\n",
    "        list_of_points_face= np.round(list_of_points_face).astype(int)\n",
    "\n",
    "        for (x1, y1) in list_of_points_face:\n",
    "\n",
    "             p_img = img[y1-kernel_size:y1+kernel_size ,x1-kernel_size:x1+kernel_size]\n",
    "\n",
    "             patches.append(p_img)  \n",
    "\n",
    "        patches = np.array(patches,dtype =np.float32)\n",
    "        torch.from_numpy(patches).to(device)\n",
    "        features = DenseNet_feature_extractor(patches) \n",
    "        #print(patches.shape)\n",
    "        #print(patches.dtype)\n",
    "        vertices = torch.tensor(np.array(features), dtype=torch.float).contiguous()   \n",
    "\n",
    "        data = Data(x=vertices, edge_index=edges,y =label)\n",
    "        torch.save(data, os.path.dirname(path)+'/'+os.path.basename(path)+'.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Wav2Lip)",
   "language": "python",
   "name": "wav2lip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
