{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39933d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch_geometric\n",
    "from PIL import Image\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import cv2\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import glob\n",
    "import os\n",
    "import math \n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import seaborn as sb\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236254c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 19:30:09.707682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:09.710223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:09.710773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:09.711429: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 19:30:09.712078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:09.712627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:09.713169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:10.896020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:10.896571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:10.897082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-19 19:30:10.897580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21597 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "kernel_size = 20\n",
    "\n",
    "\n",
    "\n",
    "model = DenseNet121(input_shape=(kernel_size*2,kernel_size*2,3),weights=None, include_top=False)\n",
    "\n",
    "# Choose the desired low-level feature layers\n",
    "low_level_layers = [\n",
    "    model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "    model.get_layer('conv3_block12_concat')# Output of the last layer in the second dense block\n",
    "]\n",
    "# low_level_layers = [\n",
    "#     model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "#     model.get_layer('conv3_block12_concat'), \n",
    "#     model.get_layer('conv4_block24_concat')# Output of the last layer in the second dense block\n",
    "# ]\n",
    "\n",
    "# Create a new model with the desired low-level feature layers as outputs\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=[layer.output for layer in low_level_layers])\n",
    "feature_extractor.load_weights(\"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.h5\")\n",
    "feature_extractor.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4594913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "kernel_size = 20\n",
    "\n",
    "\n",
    "\n",
    "model = DenseNet121(input_shape=(kernel_size*2,kernel_size*2,3),weights=None, include_top=False)\n",
    "\n",
    "# Choose the desired low-level feature layers\n",
    "low_level_layers = [\n",
    "    model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "    model.get_layer('conv3_block12_concat')# Output of the last layer in the second dense block\n",
    "]\n",
    "# low_level_layers = [\n",
    "#     model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "#     model.get_layer('conv3_block12_concat'), \n",
    "#     model.get_layer('conv4_block24_concat')# Output of the last layer in the second dense block\n",
    "# ]\n",
    "\n",
    "# Create a new model with the desired low-level feature layers as outputs\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=[layer.output for layer in low_level_layers])\n",
    "feature_extractor.load_weights(\"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.h5\")\n",
    "feature_extractor.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27765dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input_image = tf.random.uniform((10,40,40,3))\n",
    "def DenseNet_feature_extractor(input_image) :\n",
    "    #input_image = tf.keras.preprocessing.image.img_to_array(input_image)\n",
    "    #print(\"input shape\", input_image.shape)\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    input_image = tf.keras.applications.densenet.preprocess_input(input_image)\n",
    "    #input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    feature_extracted = feature_extractor.predict(input_image,verbose=False)\n",
    "    #print(feature_extracted[0].shape,feature_extracted[1].shape)\n",
    "    global_pooling = tf.keras.layers.GlobalAveragePooling2D()(feature_extracted[0])\n",
    "    flatten = tf.keras.layers.Flatten()(global_pooling)\n",
    "    total_feature =flatten\n",
    "    for layer_cnt in range(1,2) : \n",
    "        global_pooling = tf.keras.layers.GlobalAveragePooling2D()(feature_extracted[layer_cnt])\n",
    "        flatten = tf.keras.layers.Flatten()(global_pooling)\n",
    "        total_feature = tf.concat([flatten,total_feature],1)\n",
    "    return total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d2ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size= 20\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "connections = mp_face_mesh.FACEMESH_TESSELATION\n",
    "edges = []\n",
    "for connection in connections:\n",
    "    edge = [connection[0], connection[1]]\n",
    "    edges.append(edge)\n",
    "    edge  =[connection[1], connection[0]]\n",
    "    edges.append(edge)\n",
    "edges = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85174790",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/casia-ds/preposess/*/cropped*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a6e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_float(f_min, f_max):\n",
    "    return f_min + (f_max-f_min) * random.random()\n",
    "\n",
    "class Contrast_and_Brightness(object):\n",
    "\n",
    "    def __call__(self, image_x):\n",
    "        gamma = random.randint(-40, 40)\n",
    "        alpha = random_float(0.5, 1.5)\n",
    "        image_x = cv2.addWeighted(image_x, alpha, image_x, 0, gamma)\n",
    "        \n",
    "        return image_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b53c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_contranst_bright = Contrast_and_Brightness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd643a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e653e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|                              | 0/22296 [00:00<?, ?it/s]2024-09-19 19:30:21.039717: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2024-09-19 19:30:21.562308: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-19 19:30:21.562792: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-19 19:30:21.562806: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-09-19 19:30:21.563429: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-19 19:30:21.563474: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "Processing images: 100%|██████████████████| 22296/22296 [48:43<00:00,  7.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(paths, desc=\"Processing images\"):        \n",
    "        NAME_LIST = path.split(\"/\")[-2].split(\"_\")\n",
    "        \n",
    "        #print(face_path,NAME_LIST)\n",
    "        \n",
    "        if \"live\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"real\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"spoof\" in NAME_LIST :\n",
    "            label = 0\n",
    "        elif \"attack\" in NAME_LIST :\n",
    "            label = 0        \n",
    "        else :\n",
    "            raise 0\n",
    "            print(NAME_LIST)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img = arg_contranst_bright(img)\n",
    "\n",
    "        #print(img.shape)\n",
    "\n",
    "        landmark_path = os.path.dirname(path) + \"/\"+os.path.basename(path).split(\".\")[0]+\".npy\"\n",
    "\n",
    "        list_of_points_face = np.load(landmark_path,allow_pickle=True).item()['landmark']\n",
    "        #print(list_of_points_face)\n",
    "\n",
    "        list_of_points_face = np.clip(list_of_points_face,0,256)\n",
    "\n",
    "        list_of_points_face = list_of_points_face + np.array([kernel_size,kernel_size])\n",
    "\n",
    "        img = np.pad(img, ((kernel_size, kernel_size), (kernel_size, kernel_size), (0, 0)),\n",
    "                 mode='constant', constant_values=0) \n",
    "\n",
    "\n",
    "        patches= []\n",
    "\n",
    "        list_of_points_face= np.round(list_of_points_face).astype(int)\n",
    "\n",
    "        for (x1, y1) in list_of_points_face:\n",
    "\n",
    "             p_img = img[y1-kernel_size:y1+kernel_size ,x1-kernel_size:x1+kernel_size]\n",
    "\n",
    "             patches.append(p_img)  \n",
    "\n",
    "        patches = np.array(patches,dtype =np.float32)\n",
    "        features = DenseNet_feature_extractor(patches) \n",
    "        #print(patches.shape)\n",
    "        #print(patches.dtype)\n",
    "        vertices = torch.tensor(np.array(features), dtype=torch.float).contiguous()   \n",
    "\n",
    "        data = Data(x=vertices, edge_index=edges,y =label)\n",
    "        torch.save(data, os.path.dirname(path)+'/arg_contrast_bright_'+os.path.basename(path)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f5b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/oulu/preposess/*/cropped*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b6060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████████| 136334/136334 [4:57:31<00:00,  7.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(paths, desc=\"Processing images\"):        \n",
    "        NAME_LIST = path.split(\"/\")[-2].split(\"_\")\n",
    "        \n",
    "        #print(face_path,NAME_LIST)\n",
    "        \n",
    "        if \"live\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"real\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"spoof\" in NAME_LIST :\n",
    "            label = 0\n",
    "        elif \"attack\" in NAME_LIST :\n",
    "            label = 0        \n",
    "        else :\n",
    "            raise 0\n",
    "            print(NAME_LIST)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img = arg_contranst_bright(img)\n",
    "\n",
    "        #print(img.shape)\n",
    "\n",
    "        landmark_path = os.path.dirname(path) + \"/\"+os.path.basename(path).split(\".\")[0]+\".npy\"\n",
    "\n",
    "        list_of_points_face = np.load(landmark_path,allow_pickle=True).item()['landmark']\n",
    "        #print(list_of_points_face)\n",
    "\n",
    "        list_of_points_face = np.clip(list_of_points_face,0,256)\n",
    "\n",
    "        list_of_points_face = list_of_points_face + np.array([kernel_size,kernel_size])\n",
    "\n",
    "        img = np.pad(img, ((kernel_size, kernel_size), (kernel_size, kernel_size), (0, 0)),\n",
    "                 mode='constant', constant_values=0) \n",
    "\n",
    "\n",
    "        patches= []\n",
    "\n",
    "        list_of_points_face= np.round(list_of_points_face).astype(int)\n",
    "\n",
    "        for (x1, y1) in list_of_points_face:\n",
    "\n",
    "             p_img = img[y1-kernel_size:y1+kernel_size ,x1-kernel_size:x1+kernel_size]\n",
    "\n",
    "             patches.append(p_img)  \n",
    "\n",
    "        patches = np.array(patches,dtype =np.float32)\n",
    "        features = DenseNet_feature_extractor(patches) \n",
    "        #print(patches.shape)\n",
    "        #print(patches.dtype)\n",
    "        vertices = torch.tensor(np.array(features), dtype=torch.float).contiguous()   \n",
    "\n",
    "        data = Data(x=vertices, edge_index=edges,y =label)\n",
    "        torch.save(data, os.path.dirname(path)+'/arg_contrast_bright_'+os.path.basename(path)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674a2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/Replay_attack_dataset/preposess/*/cropped*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949bad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████| 61989/61989 [2:14:57<00:00,  7.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(paths, desc=\"Processing images\"):        \n",
    "        NAME_LIST = path.split(\"/\")[-2].split(\"_\")\n",
    "        \n",
    "        #print(face_path,NAME_LIST)\n",
    "        \n",
    "        if \"live\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"real\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"spoof\" in NAME_LIST :\n",
    "            label = 0\n",
    "        elif \"attack\" in NAME_LIST :\n",
    "            label = 0        \n",
    "        else :\n",
    "            raise 0\n",
    "            print(NAME_LIST)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img = arg_contranst_bright(img)\n",
    "\n",
    "        #print(img.shape)\n",
    "\n",
    "        landmark_path = os.path.dirname(path) + \"/\"+os.path.basename(path).split(\".\")[0]+\".npy\"\n",
    "\n",
    "        list_of_points_face = np.load(landmark_path,allow_pickle=True).item()['landmark']\n",
    "        #print(list_of_points_face)\n",
    "\n",
    "        list_of_points_face = np.clip(list_of_points_face,0,256)\n",
    "\n",
    "        list_of_points_face = list_of_points_face + np.array([kernel_size,kernel_size])\n",
    "\n",
    "        img = np.pad(img, ((kernel_size, kernel_size), (kernel_size, kernel_size), (0, 0)),\n",
    "                 mode='constant', constant_values=0) \n",
    "\n",
    "\n",
    "        patches= []\n",
    "\n",
    "        list_of_points_face= np.round(list_of_points_face).astype(int)\n",
    "\n",
    "        for (x1, y1) in list_of_points_face:\n",
    "\n",
    "             p_img = img[y1-kernel_size:y1+kernel_size ,x1-kernel_size:x1+kernel_size]\n",
    "\n",
    "             patches.append(p_img)  \n",
    "\n",
    "        patches = np.array(patches,dtype =np.float32)\n",
    "        features = DenseNet_feature_extractor(patches) \n",
    "        #print(patches.shape)\n",
    "        #print(patches.dtype)\n",
    "        vertices = torch.tensor(np.array(features), dtype=torch.float).contiguous()   \n",
    "\n",
    "        data = Data(x=vertices, edge_index=edges,y =label)\n",
    "        torch.save(data, os.path.dirname(path)+'/arg_contrast_bright_'+os.path.basename(path)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5ff3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/MSU_MSFD_ds/MSU_MFSD/preposess/*/cropped*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d5f137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████████████| 15549/15549 [34:03<00:00,  7.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(paths, desc=\"Processing images\"):        \n",
    "        NAME_LIST = path.split(\"/\")[-2].split(\"_\")\n",
    "        \n",
    "        #print(face_path,NAME_LIST)\n",
    "        \n",
    "        if \"live\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"real\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"spoof\" in NAME_LIST :\n",
    "            label = 0\n",
    "        elif \"attack\" in NAME_LIST :\n",
    "            label = 0        \n",
    "        else :\n",
    "            raise 0\n",
    "            print(NAME_LIST)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img = arg_contranst_bright(img)\n",
    "\n",
    "        #print(img.shape)\n",
    "\n",
    "        landmark_path = os.path.dirname(path) + \"/\"+os.path.basename(path).split(\".\")[0]+\".npy\"\n",
    "\n",
    "        list_of_points_face = np.load(landmark_path,allow_pickle=True).item()['landmark']\n",
    "        #print(list_of_points_face)\n",
    "\n",
    "        list_of_points_face = np.clip(list_of_points_face,0,256)\n",
    "\n",
    "        list_of_points_face = list_of_points_face + np.array([kernel_size,kernel_size])\n",
    "\n",
    "        img = np.pad(img, ((kernel_size, kernel_size), (kernel_size, kernel_size), (0, 0)),\n",
    "                 mode='constant', constant_values=0) \n",
    "\n",
    "\n",
    "        patches= []\n",
    "\n",
    "        list_of_points_face= np.round(list_of_points_face).astype(int)\n",
    "\n",
    "        for (x1, y1) in list_of_points_face:\n",
    "\n",
    "             p_img = img[y1-kernel_size:y1+kernel_size ,x1-kernel_size:x1+kernel_size]\n",
    "\n",
    "             patches.append(p_img)  \n",
    "\n",
    "        patches = np.array(patches,dtype =np.float32)\n",
    "        features = DenseNet_feature_extractor(patches) \n",
    "        #print(patches.shape)\n",
    "        #print(patches.dtype)\n",
    "        vertices = torch.tensor(np.array(features), dtype=torch.float).contiguous()   \n",
    "\n",
    "        data = Data(x=vertices, edge_index=edges,y =label)\n",
    "        torch.save(data, os.path.dirname(path)+'/arg_contrast_bright_'+os.path.basename(path)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40106492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Wav2Lip)",
   "language": "python",
   "name": "wav2lip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
