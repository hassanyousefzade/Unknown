{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca228a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch_geometric\n",
    "from PIL import Image\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import clip\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import cv2\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import glob\n",
    "import os\n",
    "import math \n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import seaborn as sb\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036d170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size= 20\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "connections = mp_face_mesh.FACEMESH_TESSELATION\n",
    "edges = []\n",
    "for connection in connections:\n",
    "    edge = [connection[0], connection[1]]\n",
    "    edges.append(edge)\n",
    "    edge  =[connection[1], connection[0]]\n",
    "    edges.append(edge)\n",
    "edges = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1efc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/16\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e28cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyCLIP(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        self.conv1 = original_model.visual.conv1\n",
    "        self.ln_pre = original_model.visual.ln_pre\n",
    "        self.class_embedding = original_model.visual.class_embedding\n",
    "        self.positional_embedding = original_model.visual.positional_embedding\n",
    "        self.transformer = original_model.visual.transformer.resblocks[:2]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.cat(\n",
    "            [self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x],\n",
    "            dim=1\n",
    "        )\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        return x.mean(dim=0)\n",
    "first_two_blocks = EarlyCLIP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8be322",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/media/hassan-hossein/B660FEE360FEA8EF/oulu/preposess/*/cropped*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240fd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|███████████████████████████████████████████████████████████████████████████| 136334/136334 [6:55:58<00:00,  5.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(paths, desc=\"Processing images\"):        \n",
    "        NAME_LIST = path.split(\"/\")[-2].split(\"_\")\n",
    "        \n",
    "        #print(face_path,NAME_LIST)\n",
    "        if os.path.exists( os.path.dirname(path)+'/clip_'+os.path.basename(path)+'.pt'):\n",
    "            continue\n",
    "        \n",
    "        if \"live\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"real\" in NAME_LIST :\n",
    "            label = 1\n",
    "        elif \"spoof\" in NAME_LIST :\n",
    "            label = 0\n",
    "        elif \"attack\" in NAME_LIST :\n",
    "            label = 0        \n",
    "        else :\n",
    "            raise 0\n",
    "            print(NAME_LIST)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #print(img.shape)\n",
    "\n",
    "        landmark_path = os.path.dirname(path) + \"/\"+os.path.basename(path).split(\".\")[0]+\".npy\"\n",
    "\n",
    "        list_of_points_face = np.load(landmark_path,allow_pickle=True).item()['landmark']\n",
    "        #print(list_of_points_face)\n",
    "\n",
    "        list_of_points_face = np.clip(list_of_points_face,0,256)\n",
    "\n",
    "        list_of_points_face = list_of_points_face + np.array([kernel_size,kernel_size])\n",
    "\n",
    "        img = np.pad(img, ((kernel_size, kernel_size), (kernel_size, kernel_size), (0, 0)),\n",
    "                 mode='constant', constant_values=0) \n",
    "\n",
    "\n",
    "        patches= []\n",
    "\n",
    "        list_of_points_face= np.round(list_of_points_face).astype(int)\n",
    "\n",
    "        for (x1, y1) in list_of_points_face:\n",
    "\n",
    "             p_img = img[y1-kernel_size:y1+kernel_size ,x1-kernel_size:x1+kernel_size]\n",
    "             p_img = Image.fromarray(p_img)\n",
    "             p_img= preprocess(p_img)\n",
    "             p_img = np.array(p_img)\n",
    "             patches.append(p_img)\n",
    "\n",
    "        patches = np.array(patches,dtype =np.float32)\n",
    "        patches = torch.tensor(patches)\n",
    "        patches = patches.to(\"cuda\").half() \n",
    "        with torch.no_grad():\n",
    "                features = first_two_blocks(patches).float()\n",
    "            #np.save(os.path.dirname(path) + \"/\" + \"CLIP_\" + os.path.basename(path) + \".npy\", image_features.cpu().numpy())        \n",
    "        #print(features.shape)\n",
    "        vertices = torch.tensor(np.array(features.cpu()), dtype=torch.float).contiguous()   \n",
    "\n",
    "        data = Data(x=vertices, edge_index=edges,y =label)\n",
    "        #torch.save(data, os.path.dirname(path)+'/clip_'+os.path.basename(path)+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec15db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Wav2Lip)",
   "language": "python",
   "name": "wav2lip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
