{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2711b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import GPUtil\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import os\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "from performance import performances_val\n",
    "from PIL import Image\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.data import Data\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import ChebConv, GraphSAGE,GraphUNet ,TransformerConv\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "seed = 1378\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "landmark_colors = np.load(\"landmark_colors.npy\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_list=[]\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.expand_dims(landmark_colors,axis=1))  \n",
    "for x in landmark_colors:\n",
    "    one_hot_list.append(enc.transform([[x]]).toarray()[0])\n",
    "one_hot_arr  = np.array(one_hot_list)\n",
    "one_hot_arr = torch.FloatTensor(one_hot_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e46abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class landmark_dataset(Dataset):\n",
    "    def __init__(self, info_list, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.graphs = pd.read_csv(info_list, delimiter=\",\", header=None)\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        graph_path = self.graphs.iloc[idx]\n",
    "        graph_path = graph_path[0]\n",
    "        face_path = graph_path[:-3]\n",
    "        \n",
    "        face = cv2.imread(face_path)\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        face = np.array(face,dtype =np.float32)\n",
    "        face_features = DenseNet_feature_extractor_onnx(face)\n",
    "        face_feature = torch.tensor(np.array(face_features), dtype=torch.float)\n",
    "        \n",
    "        data = torch.load(graph_path)\n",
    "        #data.x =data.x.t()\n",
    "        return Data(x=data.x, edge_index=data.edge_index,y =data.y,face_feature=face_feature) \n",
    "class landmark_dataset_train(Dataset):\n",
    "    def __init__(self, info_list, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.graphs = pd.read_csv(info_list, delimiter=\",\", header=None)\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        graph_path = self.graphs.iloc[idx]\n",
    "        graph_path = graph_path[0]\n",
    "        \n",
    "        \n",
    "        face_path = graph_path[:-3]\n",
    "        \n",
    "        face = cv2.imread(face_path)\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        face = np.array(face,dtype =np.float32)\n",
    "        \n",
    "        #face = np.expand_dims(face,axis=0)\n",
    "        #print(face.shape)\n",
    "        face = torch.tensor(face)\n",
    "        \n",
    "        #face = face.permute(2, 0, 1)\n",
    "        \n",
    "        face = torch.unsqueeze(face,dim =-1)\n",
    "        \n",
    "        \n",
    "        data = torch.load(graph_path)\n",
    "        \n",
    "        domain_name = graph_path.split(\"/\")[-4]\n",
    "        \n",
    "            \n",
    "        if domain_name == \"MSU_MFSD\" :\n",
    "            \n",
    "            domain_label = torch.FloatTensor([[0,0,1]])\n",
    "            \n",
    "        elif domain_name == \"Replay_attack_dataset\" :\n",
    "            \n",
    "            domain_label = torch.FloatTensor([[0,1,0]])\n",
    "            \n",
    "        elif domain_name == \"oulu\" :\n",
    "            \n",
    "            domain_label = torch.FloatTensor([[0,0,1]])\n",
    "        else :\n",
    "            print(graph_path,domain_name )\n",
    "            raise \"error\"\n",
    "            \n",
    "        \n",
    "        data = Data(x=data.x, edge_index=data.edge_index,y =data.y,data = domain_label,y_node = one_hot_arr,face=face)\n",
    "\n",
    "        #data.x =data.x.t()\n",
    "        return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf76228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassan-hossein/.local/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 640)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "providers = ['CUDAExecutionProvider']\n",
    "\n",
    "def preprocess_input_numpy(input_images):\n",
    "    # اطمینان از اینکه تصاویر به float32 تبدیل شده‌اند\n",
    "    input_images = input_images.astype(np.float32)\n",
    "    \n",
    "    # تبدیل محدوده پیکسل‌ها از [0, 255] به [0, 1]\n",
    "    input_images /= 255.0\n",
    "\n",
    "    # تبدیل محدوده پیکسل‌ها از [0, 1] به [-1, 1]\n",
    "    input_images = input_images * 2.0 - 1.0\n",
    "\n",
    "    return input_images\n",
    "\n",
    "\n",
    "def DenseNet_feature_extractor_onnx(input_images):\n",
    "    # پیش‌پردازش ورودی\n",
    "    input_images = preprocess_input_numpy(input_images)\n",
    "\n",
    "    # بررسی اینکه ورودی چهار بعدی است\n",
    "    assert input_images.ndim == 4, \"Input must have 4 dimensions (batch_size, height, width, channels)\"\n",
    "\n",
    "    # تهیه دیکشنری ورودی برای ONNX\n",
    "    inputs = {ort_session.get_inputs()[0].name: input_images}\n",
    "    \n",
    "    # انجام پیش‌بینی\n",
    "    onnx_outputs = ort_session.run(None, inputs)\n",
    "    \n",
    "    # استخراج ویژگی‌ها از خروجی‌های ONNX\n",
    "    feature_extracted_0 = onnx_outputs[0]\n",
    "    feature_extracted_1 = onnx_outputs[1]\n",
    "\n",
    "    # محاسبه Global Average Pooling و Flatten برای هر تصویر در مینی‌بتچ\n",
    "    global_pooling_0 = np.mean(feature_extracted_0, axis=(1, 2))\n",
    "    global_pooling_1 = np.mean(feature_extracted_1, axis=(1, 2))\n",
    "\n",
    "    flatten_0 = global_pooling_0.reshape(global_pooling_0.shape[0], -1)\n",
    "    flatten_1 = global_pooling_1.reshape(global_pooling_1.shape[0], -1)\n",
    "\n",
    "    # ادغام ویژگی‌ها\n",
    "    total_feature = np.concatenate([flatten_0, flatten_1], axis=1)\n",
    "    return total_feature\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# input_image = tf.random.uniform((256, 256, 3))  # Replace with actual image data\n",
    "# features = DenseNet_feature_extractor_onnx(input_image)\n",
    "# print(features.shape)\n",
    "\n",
    "\n",
    "# بارگذاری مدل ONNX\n",
    "onnx_model_path = \"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path , providers=providers)\n",
    "\n",
    "# تابعی برای پیش‌پردازش ورودی مشابه DenseNet اما با استفاده از NumPy\n",
    "\n",
    "# تابعی برای استفاده از مدل ONNX\n",
    "\n",
    "# مثال استفاده\n",
    "input_images = np.random.uniform(0, 255, (10, 256, 256, 3)).astype(np.uint8)  # جایگزین با داده واقعی\n",
    "features = DenseNet_feature_extractor_onnx(input_images)\n",
    "print(features.shape)  # باید به شکل (10, feature_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a535b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [(\"CUDAExecutionProvider\", {\"device_id\": torch.cuda.current_device(),\n",
    "                                        \"user_compute_stream\": str(torch.cuda.current_stream().cuda_stream)})]\n",
    "sess_options = ort.SessionOptions()\n",
    "sess = ort.InferenceSession(onnx_model_path, sess_options=sess_options, providers=providers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98bcf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5c8dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['CPUExecutionProvider']\n",
      "Current provider: {'CPUExecutionProvider': {}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# تنظیمات برای استفاده از GPU\n",
    "\n",
    "\n",
    "# # ایجاد جلسه استنتاج با استفاده از GPU\n",
    "# ort_session = ort.InferenceSession(\"path_to_your_model.onnx\", providers=providers)\n",
    "\n",
    "# بررسی اینکه آیا GPU استفاده می‌شود یا خیر\n",
    "print(\"Available providers:\", ort_session.get_providers())\n",
    "print(\"Current provider:\", ort_session.get_provider_options())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46427f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "print(sess.get_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de14d0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# تنظیمات برای استفاده از GPU\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "# ایجاد جلسه استنتاج با استفاده از GPU\n",
    "ort_session = ort.InferenceSession(onnx_model_path , providers=providers)\n",
    "\n",
    "print(\"Available providers:\", ort_session.get_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True :\n",
    "    features = DenseNet_feature_extractor_onnx(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# فرض کنید که داده‌های ورودی شما یک توالی از ۶۸۰ ویژگی دارد.\n",
    "#sequence_length = 128\n",
    "#feature_dim = 4  # هر ویژگی تک‌بعدی است\n",
    "\n",
    "# نمونه داده ورودی (تعداد نمونه‌ها × طول توالی × بعد ویژگی)\n",
    "#inputs = torch.rand(32, sequence_length, feature_dim)  # 32 نمونه با 680 ویژگی تک‌بعدی\n",
    "\n",
    "# تعریف یک لایه ترنسفورمر انکودر\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_layers, nhead):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.linear = nn.Linear(1, embed_dim)  # افزایش بعد ویژگی‌ها\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(768 * embed_dim, 1)  # فرضا 10 کلاس خروجی داریم\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)  # تغییر بعد ویژگی‌ها\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # فشرده‌سازی به یک بردار برای طبقه‌بندی\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# # ایجاد مدل و ارسال ورودی\n",
    "# # model = SimpleTransformer(embed_dim=embed_dim, num_layers=2, nhead=4)\n",
    "# # output = model(inputs)\n",
    "\n",
    "# #print(output.shape)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# فرض کنید که داده‌های ورودی شما یک توالی از ۶۸۰ ویژگی دارد.\n",
    "sequence_length = 128\n",
    "feature_dim = 1  # هر ویژگی تک‌بعدی است\n",
    "\n",
    "# نمونه داده ورودی (تعداد نمونه‌ها × طول توالی × بعد ویژگی)\n",
    "#inputs = torch.rand(32, sequence_length, feature_dim)  # 32 نمونه با 680 ویژگی تک‌بعدی\n",
    "\n",
    "# تعریف یک لایه ترنسفورمر انکودر\n",
    "class SimpleTransformer_2(nn.Module):\n",
    "    def __init__(self, embed_dim, num_layers, nhead):\n",
    "        super(SimpleTransformer_2, self).__init__()\n",
    "        self.linear = nn.Linear(feature_dim, embed_dim)  # افزایش بعد ویژگی‌ها\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(sequence_length * embed_dim, 7)  # فرضا 10 کلاس خروجی داریم\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)  # تغییر بعد ویژگی‌ها\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # فشرده‌سازی به یک بردار برای طبقه‌بندی\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# # ایجاد مدل و ارسال ورودی\n",
    "# # model = SimpleTransformer(embed_dim=embed_dim, num_layers=2, nhead=4)\n",
    "# # output = model(inputs)\n",
    "\n",
    "# #print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280237d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708db2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = landmark_dataset_train(\"/home/hassan-hossein/single_image_graph_face_anti_spoofing/train/graph_train_O_M_I.txt\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,num_workers=0)\n",
    "\n",
    "test_dataset = landmark_dataset(\"/home/hassan-hossein/single_image_graph_face_anti_spoofing/train/graph_test_C.txt\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader :\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17506414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNodeDropout(nn.Module):\n",
    "    def __init__(self, drop_prob=0.3):\n",
    "        super(RandomNodeDropout, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x, enabled=True):\n",
    "        if not self.training or not enabled:\n",
    "            return x\n",
    "        \n",
    "        num_rows = x.size(0)\n",
    "        mask = torch.rand(num_rows, device=x.device) > self.drop_prob\n",
    "        mask = mask.float().unsqueeze(1).expand_as(x)\n",
    "        x = x * mask\n",
    "        \n",
    "        return x\n",
    "\n",
    "soft = torch.nn.Softmax(dim=1)\n",
    "\n",
    "num_domain = 3\n",
    "class GRL(nn.Module):\n",
    "\n",
    "    def __init__(self, max_iter):\n",
    "        super(GRL, self).__init__()\n",
    "        self.iter_num = 0\n",
    "        self.alpha = 10\n",
    "        self.low = 0.0\n",
    "        self.high = 1.0\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.iter_num += 1\n",
    "        return input * 1.0\n",
    "\n",
    "    def backward(self, gradOutput):\n",
    "        coeff = np.float(2.0 * (self.high - self.low) / (1.0 + np.exp(-self.alpha * self.iter_num / self.max_iter))\n",
    "                         - (self.high - self.low) + self.low)\n",
    "        return -coeff*10* gradOutput\n",
    "\n",
    "num_node  =train_dataset.num_node_features\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels ,max_iter):\n",
    "        super(Discriminator, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphSAGE(hidden_channels, hidden_channels,2,dropout=0.5)\n",
    "        self.conv2 = GraphSAGE(hidden_channels, hidden_channels,2,dropout=0.5)\n",
    "        self.conv3 = GraphSAGE(hidden_channels, hidden_channels,2,dropout=0.5)\n",
    "        self.conv4 = GraphSAGE(hidden_channels, hidden_channels,2,dropout=0.5)\n",
    "        self.drop_1 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.drop_2 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.grl_layer = GRL(max_iter)\n",
    "        self.fc2 = nn.Linear(hidden_channels, num_domain)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch,drop_en):\n",
    "        #print(\"graph fea :\",graph_level_feature.shape ,edge_index.shape )\n",
    "        \n",
    "        x = self.grl_layer(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.drop_1(x,drop_en)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.drop_2(x,drop_en)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "         \n",
    "        return x \n",
    "class Discriminator_G(nn.Module):\n",
    "    def __init__(self, max_iter):\n",
    "        super(Discriminator_G, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, num_domain)\n",
    "        self.ad_net = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            self.fc2\n",
    "        )\n",
    "        self.grl_layer = GRL(max_iter)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        adversarial_out = self.ad_net(self.grl_layer(feature))\n",
    "        return adversarial_out\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels,max_iter=4000):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = ChebConv(num_node, hidden_channels,2)\n",
    "        self.conv2 = ChebConv(hidden_channels, hidden_channels,2)\n",
    "        self.conv3 = ChebConv(hidden_channels, hidden_channels,2)\n",
    "        self.conv4 = ChebConv(hidden_channels, hidden_channels,2)\n",
    "        self.conv5 = ChebConv(hidden_channels*2, hidden_channels,2)\n",
    "        self.conv6 = ChebConv(hidden_channels, hidden_channels,2)\n",
    "        self.conv7 = ChebConv(hidden_channels, hidden_channels,2)\n",
    "        self.conv8 = ChebConv(hidden_channels, hidden_channels,2)\n",
    "        self.lin = SimpleTransformer(8,5,8)\n",
    "        \n",
    "        self.drop_1 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.drop_2 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.drop_3 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.drop_4 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.drop_5 = RandomNodeDropout(drop_prob=0.3)\n",
    "        self.drop_6 = RandomNodeDropout(drop_prob=0.3)\n",
    "        \n",
    "        \n",
    "        self.b1 = BatchNorm(hidden_channels)\n",
    "        self.b2 = BatchNorm(hidden_channels)\n",
    "        self.b3 = BatchNorm(hidden_channels)\n",
    "        self.b4 = BatchNorm(hidden_channels)\n",
    "        self.b5 = BatchNorm(hidden_channels)\n",
    "        self.b6 = BatchNorm(hidden_channels)\n",
    "        self.b7 = BatchNorm(hidden_channels)\n",
    "        \n",
    "        self.dis = Discriminator(hidden_channels,max_iter)\n",
    "        self.dis_g = Discriminator_G(max_iter)\n",
    "        \n",
    "        self.conv1_node_class = GraphSAGE(hidden_channels, hidden_channels,2,dropout=0.5)\n",
    "        \n",
    "        self.conv2_node_class = GraphSAGE(hidden_channels, hidden_channels,2,dropout=0.5)\n",
    "        \n",
    "        self.lin_node_class = SimpleTransformer_2(64,10,8)\n",
    "        \n",
    "\n",
    "        \n",
    "        #self.tcn = Simple1DCNN()\n",
    "        \n",
    "    def forward(self, x, edge_index, batch,drop_en,index_arr,face_feature):\n",
    "        #print(\"graph fea :\",graph_level_feature.shape ,edge_index.shape )\n",
    "        \n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x  = self.b1(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.drop_1(x,drop_en)\n",
    "        \n",
    "        \n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x  = self.b2(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.drop_2(x,drop_en)\n",
    "                \n",
    "        dicriminator = x\n",
    "        \n",
    "        x_1 = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x  = self.b3(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.drop_3(x,drop_en)\n",
    "        \n",
    "        x_3 = global_mean_pool(x, batch)\n",
    "        \n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        x  = self.b4(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.drop_4(x,drop_en)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((x,dicriminator),dim=1) #skip connection\n",
    "        \n",
    "        x = self.conv5(x, edge_index)\n",
    "        x  = self.b5(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.drop_5(x,drop_en)\n",
    "        \n",
    "        x_5 = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = self.conv6(x, edge_index)\n",
    "        x  = self.b6(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.drop_6(x,drop_en)\n",
    "        \n",
    "        x = self.conv7(x, edge_index)\n",
    "        x  = self.b7(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        \n",
    "        x = self.conv8(x, edge_index)\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat((x,face_feature),dim=1)\n",
    "                \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        dis_g  = self.dis_g(x)\n",
    "        \n",
    "        x = torch.unsqueeze(x,2)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        x= x.sigmoid()\n",
    "        \n",
    "        \n",
    "        node_class_hidden = self.conv1_node_class(dicriminator,edge_index)\n",
    "        node_class_hidden = node_class_hidden.relu()\n",
    "        \n",
    "        node_class_hidden = self.conv2_node_class(node_class_hidden,edge_index)\n",
    "        node_class_hidden = node_class_hidden.relu()\n",
    "        \n",
    "        #print(index_arr.shape , node_class_hidden.shape)\n",
    "        \n",
    "        node_class_hidden = node_class_hidden[index_arr]\n",
    "        \n",
    "        #print(node_class_hidden.shape)\n",
    "        node_class_hidden = torch.unsqueeze(node_class_hidden, dim=2)\n",
    "        \n",
    "        node_pre = self.lin_node_class(node_class_hidden)\n",
    "        \n",
    "            \n",
    "        #print(\"hiiii :\" , dicriminator.shape)\n",
    "        dis_invariant = self.dis(dicriminator,edge_index,batch,drop_en)\n",
    "        \n",
    "        return x ,dis_invariant ,node_pre , dis_g\n",
    "\n",
    "model = GCN(hidden_channels=128).to(device)\n",
    "print(model)\n",
    "\n",
    "def check_folder(log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion_domain_G = torch.nn.CrossEntropyLoss()\n",
    "criterion= nn.BCELoss()\n",
    "criterion_node_class=  torch.nn.CrossEntropyLoss()\n",
    "criterion_2 = torch.nn.CrossEntropyLoss()\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    losses_adv_loss=0\n",
    "    losses_adv_g_loss=0\n",
    "    losses_loss_cls=0\n",
    "    total_loss = 0\n",
    "    losses_node_cls_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}', unit='batch')\n",
    "    for graph in progress_bar:  # Iterate in batches over the training dataset.\n",
    "        \n",
    "       \n",
    "        face = np.moveaxis(graph.face.numpy(), -1, 0) \n",
    "        #print(face.shape)\n",
    "        face_feature = torch.tensor(DenseNet_feature_extractor_onnx(face)).to(device)\n",
    "        graph = graph.to(device)  # Move data to the device\n",
    "        \n",
    "        drop_en = bool(np.random.binomial(1, 0.5))\n",
    "        \n",
    "        if graph.x.shape[0] < 29952 :\n",
    "            \n",
    "            if graph.x.shape[0] < 64:\n",
    "            \n",
    "                index_arr = torch.randint(0, int(graph.x.shape[0]), (int(graph.x.shape[0]),)).to(device)\n",
    "            else :\n",
    "                \n",
    "                index_arr = torch.randint(0, int(graph.x.shape[0]), (64,)).to(device)\n",
    "        else :\n",
    "                \n",
    "            index_arr = torch.randint(0, 29952, (64,)).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        out,domain_invariant,node_pre,dis_g = model(graph.x, graph.edge_index, graph.batch,drop_en,index_arr,face_feature)  # Perform a single forward pass.\n",
    "        \n",
    "        adv_loss = criterion_2(domain_invariant, graph.data)\n",
    "        adv_g_loss = criterion_domain_G(dis_g, graph.data)\n",
    "        \n",
    "        node_cls_loss = criterion_node_class(node_pre.squeeze(),graph.y_node.float()[index_arr])\n",
    "        #print(out.shape , graph.y.long().shape)\n",
    "        loss_cls = criterion(out.squeeze(), graph.y.float())  # Compute the loss.\n",
    "        \n",
    "        loss_all = adv_loss + loss_cls + node_cls_loss + adv_g_loss\n",
    "        loss_all.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        total_loss = loss_all + total_loss\n",
    "        losses_adv_loss = adv_loss + losses_adv_loss\n",
    "        losses_loss_cls = loss_cls + losses_loss_cls\n",
    "        losses_node_cls_loss = node_cls_loss + losses_node_cls_loss\n",
    "        losses_adv_g_loss  = adv_g_loss + losses_adv_g_loss\n",
    "\n",
    "    print(\"/////////////////////////\")\n",
    "    print(\"adverserial loss : \" ,losses_adv_loss/len(train_loader.dataset))  \n",
    "    print(\"classification loss : \" , losses_loss_cls/len(train_loader.dataset))\n",
    "    print(\"classification node loss : \" , losses_node_cls_loss/len(train_loader.dataset))\n",
    "    print(\"losses_adv_g loss : \" , losses_adv_g_loss/len(train_loader.dataset))\n",
    "    print(\"total loss : \" ,total_loss/len(train_loader.dataset) )\n",
    "    print(\"////////////////////////////////////\")\n",
    "def test(loader,epoch):\n",
    "    score_path = os.path.join(\"/home/hassan-hossein/single_image_graph_face_anti_spoofing/train/Scores\", \"epoch_{}\".format(epoch+1))\n",
    "    check_folder(score_path)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores_list = []\n",
    "\n",
    "        correct = 0\n",
    "        s= 0\n",
    "        progress_bar = tqdm(loader, desc=f'Epoch {epoch+1}', unit='batch')\n",
    "        for graph in progress_bar:  # Iterate in batches over the training/test dataset.\n",
    "            if graph.x.shape[0] < 29952 :\n",
    "\n",
    "                if graph.x.shape[0] < 64:\n",
    "\n",
    "                    index_arr = torch.randint(0, int(graph.x.shape[0]), (int(graph.x.shape[0]),)).to(device)\n",
    "                else :\n",
    "\n",
    "                    index_arr = torch.randint(0, int(graph.x.shape[0]), (64,)).to(device)\n",
    "            else :\n",
    "\n",
    "                index_arr = torch.randint(0, 29952, (64,)).to(device)\n",
    "                \n",
    "            face_feature = DenseNet_feature_extractor_onnx(graph.face.numpy()).to(device)    \n",
    "            graph = graph.to(device)  # Move data to the device\n",
    "            logit,_ ,_= model(graph.x, graph.edge_index, graph.batch,False,index_arr,face_feature)\n",
    "            live_label = graph.y.float()\n",
    "            for i in range(len(logit)):\n",
    "                        scores_list.append(\"{} {}\\n\".format(logit[i].item(), live_label[i].item()))\n",
    "  # Check against ground-truth labels.\n",
    "    map_score_val_filename = os.path.join(score_path, \"score.txt\")\n",
    "    with open(map_score_val_filename, 'w') as file:\n",
    "                file.writelines(scores_list)\n",
    "    print(\"score: write test scores to {}\".format(map_score_val_filename))\n",
    "    test_ACC, fpr, FRR, HTER, auc_test, test_err, tpr = performances_val(map_score_val_filename)\n",
    "    print(\"epoch:{:d}, test:  val_ACC={:.4f}, HTER={:.4f}, AUC={:.4f}, val_err={:.4f}, ACC={:.4f}, TPR={:.4f}\".format(\n",
    "        epoch + 1, test_ACC, HTER, auc_test, test_err, test_ACC, tpr))  \n",
    "    return auc_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24756095",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0\n",
    "for epoch in range(1, 1000):\n",
    "    train(epoch)\n",
    "    print(\"***************************\")\n",
    "    #print(\"train : \")\n",
    "    #train_auc = test(train_loader,epoch)\n",
    "    print(\"test : \")\n",
    "    test_auc = test(test_loader,epoch)\n",
    "    if test_auc > best_auc:\n",
    "        print(\"improve acc .. .. ..\")\n",
    "        torch.save(model.state_dict(), 'add_garph_level_V_10_without_pipline_domain_generalization_tuning.pth')\n",
    "        best_auc = test_auc\n",
    "        continue  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db758c9e",
   "metadata": {},
   "source": [
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your feature extractor model as defined in your code\n",
    "# Assuming feature_extractor is already defined as in your provided code\n",
    "\n",
    "# Specify the path where you want to save the ONNX model\n",
    "onnx_model_path = \"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.onnx\"\n",
    "\n",
    "# Convert the Keras model to ONNX format\n",
    "spec = (tf.TensorSpec((None, 256, 256, 3), tf.float32, name=\"input\"),)\n",
    "model_proto, _ = tf2onnx.convert.from_keras(feature_extractor, input_signature=spec, output_path=onnx_model_path)\n",
    "\n",
    "print(f\"Model converted to ONNX and saved at: {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a425f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "kernel_size = 20\n",
    "\n",
    "\n",
    "\n",
    "model = DenseNet121(input_shape=(256,256,3),weights=None, include_top=False)\n",
    "\n",
    "# Choose the desired low-level feature layers\n",
    "low_level_layers = [\n",
    "    model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "    model.get_layer('conv3_block12_concat')# Output of the last layer in the second dense block\n",
    "]\n",
    "# low_level_layers = [\n",
    "#     model.get_layer('conv2_block2_concat'),  # Output of the last layer in the first dense block\n",
    "#     model.get_layer('conv3_block12_concat'), \n",
    "#     model.get_layer('conv4_block24_concat')# Output of the last layer in the second dense block\n",
    "# ]\n",
    "\n",
    "# Create a new model with the desired low-level feature layers as outputs\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=[layer.output for layer in low_level_layers])\n",
    "feature_extractor.load_weights(\"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.h5\")\n",
    "feature_extractor.trainable=False\n",
    "\n",
    "\n",
    "#input_image = tf.random.uniform((10,40,40,3))\n",
    "def DenseNet_feature_extractor(input_image) :\n",
    "    #input_image = tf.keras.preprocessing.image.img_to_array(input_image)\n",
    "    #print(\"input shape\", input_image.shape)\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    input_image = tf.keras.applications.densenet.preprocess_input(input_image)\n",
    "    #input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    feature_extracted = feature_extractor.predict(input_image,verbose=False)\n",
    "    #print(feature_extracted[0].shape,feature_extracted[1].shape)\n",
    "    global_pooling = tf.keras.layers.GlobalAveragePooling2D()(feature_extracted[0])\n",
    "    flatten = tf.keras.layers.Flatten()(global_pooling)\n",
    "    total_feature =flatten\n",
    "    for layer_cnt in range(1,2) : \n",
    "        global_pooling = tf.keras.layers.GlobalAveragePooling2D()(feature_extracted[layer_cnt])\n",
    "        flatten = tf.keras.layers.Flatten()(global_pooling)\n",
    "        total_feature = tf.concat([flatten,total_feature],1)\n",
    "    return total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7046169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to ONNX and saved at: /home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.onnx\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "\n",
    "# مسیر ذخیره مدل ONNX\n",
    "onnx_model_path = \"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.onnx\"\n",
    "\n",
    "# تبدیل مدل Keras به فرمت ONNX\n",
    "spec = (tf.TensorSpec((None, 256, 256, 3), tf.float32, name=\"input\"),)\n",
    "\n",
    "# تبدیل مدل به ONNX\n",
    "model_proto, _ = tf2onnx.convert.from_keras(feature_extractor, input_signature=spec, output_path=onnx_model_path)\n",
    "\n",
    "print(f\"Model converted to ONNX and saved at: {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0914e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 640)\n"
     ]
    }
   ],
   "source": [
    "باشد\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2e7e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA devices: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80fc7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 11.6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN version incompatibility: PyTorch was compiled  against (8, 3, 2) but found runtime version (8, 1, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 3, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuDNN version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Wav2Lip/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:68\u001b[0m, in \u001b[0;36mversion\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m():\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the version of cuDNN\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __cudnn_version\n",
      "File \u001b[0;32m~/anaconda3/envs/Wav2Lip/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:50\u001b[0m, in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m ld_library_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(substring \u001b[38;5;129;01min\u001b[39;00m ld_library_path \u001b[38;5;28;01mfor\u001b[39;00m substring \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcudnn\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_error_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     51\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLooks like your LD_LIBRARY_PATH contains incompatible version of cudnn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease either remove it from the path or install cudnn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompile_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_error_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     55\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone possibility is that there is a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconflicting cuDNN in LD_LIBRARY_PATH.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN version incompatibility: PyTorch was compiled  against (8, 3, 2) but found runtime version (8, 1, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 3, 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e1af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Wav2Lip)",
   "language": "python",
   "name": "wav2lip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
