{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eceba885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 640)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "providers = ['CUDAExecutionProvider']\n",
    "\n",
    "def preprocess_input_numpy(input_images):\n",
    "    # اطمینان از اینکه تصاویر به float32 تبدیل شده‌اند\n",
    "    input_images = input_images.astype(np.float32)\n",
    "    \n",
    "    # تبدیل محدوده پیکسل‌ها از [0, 255] به [0, 1]\n",
    "    input_images /= 255.0\n",
    "\n",
    "    # تبدیل محدوده پیکسل‌ها از [0, 1] به [-1, 1]\n",
    "    input_images = input_images * 2.0 - 1.0\n",
    "\n",
    "    return input_images\n",
    "\n",
    "\n",
    "def DenseNet_feature_extractor_onnx(input_images):\n",
    "    # پیش‌پردازش ورودی\n",
    "    input_images = preprocess_input_numpy(input_images)\n",
    "\n",
    "    # بررسی اینکه ورودی چهار بعدی است\n",
    "    assert input_images.ndim == 4, \"Input must have 4 dimensions (batch_size, height, width, channels)\"\n",
    "\n",
    "    # تهیه دیکشنری ورودی برای ONNX\n",
    "    inputs = {ort_session.get_inputs()[0].name: input_images}\n",
    "    \n",
    "    # انجام پیش‌بینی\n",
    "    onnx_outputs = ort_session.run(None, inputs)\n",
    "    \n",
    "    # استخراج ویژگی‌ها از خروجی‌های ONNX\n",
    "    feature_extracted_0 = onnx_outputs[0]\n",
    "    feature_extracted_1 = onnx_outputs[1]\n",
    "\n",
    "    # محاسبه Global Average Pooling و Flatten برای هر تصویر در مینی‌بتچ\n",
    "    global_pooling_0 = np.mean(feature_extracted_0, axis=(1, 2))\n",
    "    global_pooling_1 = np.mean(feature_extracted_1, axis=(1, 2))\n",
    "\n",
    "    flatten_0 = global_pooling_0.reshape(global_pooling_0.shape[0], -1)\n",
    "    flatten_1 = global_pooling_1.reshape(global_pooling_1.shape[0], -1)\n",
    "\n",
    "    # ادغام ویژگی‌ها\n",
    "    total_feature = np.concatenate([flatten_0, flatten_1], axis=1)\n",
    "    return total_feature\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# input_image = tf.random.uniform((256, 256, 3))  # Replace with actual image data\n",
    "# features = DenseNet_feature_extractor_onnx(input_image)\n",
    "# print(features.shape)\n",
    "\n",
    "\n",
    "# بارگذاری مدل ONNX\n",
    "onnx_model_path = \"/home/hassan-hossein/graph_landmark_face_anti_spoofing/dense_net_model.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path , providers=providers)\n",
    "\n",
    "# تابعی برای پیش‌پردازش ورودی مشابه DenseNet اما با استفاده از NumPy\n",
    "\n",
    "# تابعی برای استفاده از مدل ONNX\n",
    "\n",
    "# مثال استفاده\n",
    "input_images = np.random.uniform(0, 255, (10, 256, 256, 3)).astype(np.uint8)  # جایگزین با داده واقعی\n",
    "features = DenseNet_feature_extractor_onnx(input_images)\n",
    "print(features.shape)  # باید به شکل (10, feature_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5af99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# تنظیمات برای استفاده از GPU\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "# ایجاد جلسه استنتاج با استفاده از GPU\n",
    "ort_session = ort.InferenceSession(onnx_model_path , providers=providers)\n",
    "\n",
    "print(\"Available providers:\", ort_session.get_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6da849",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m :\n\u001b[0;32m----> 2\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mDenseNet_feature_extractor_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mDenseNet_feature_extractor_onnx\u001b[0;34m(input_images)\u001b[0m\n\u001b[1;32m     28\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {ort_session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname: input_images}\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# انجام پیش‌بینی\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m onnx_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# استخراج ویژگی‌ها از خروجی‌های ONNX\u001b[39;00m\n\u001b[1;32m     34\u001b[0m feature_extracted_0 \u001b[38;5;241m=\u001b[39m onnx_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/onnx_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:200\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True :\n",
    "    features = DenseNet_feature_extractor_onnx(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be060be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "# import os\n",
    "# from torch_geometric.data import Dataset, download_url\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# import pandas as pd\n",
    "# from performance import performances_val\n",
    "# from PIL import Image\n",
    "# from torch.nn import Linear\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "# from torch_geometric.nn.norm import BatchNorm\n",
    "# from torch_geometric.data import Data\n",
    "# from sklearn import metrics\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from torch_geometric.nn import ChebConv, GraphSAGE,GraphUNet ,TransformerConv\n",
    "# import cv2\n",
    "\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "seed = 1378\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "#landmark_colors = np.load(\"landmark_colors.npy\")\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "# one_hot_list=[]\n",
    "# enc = OneHotEncoder()\n",
    "# enc.fit(np.expand_dims(landmark_colors,axis=1))  \n",
    "# for x in landmark_colors:\n",
    "#     one_hot_list.append(enc.transform([[x]]).toarray()[0])\n",
    "# one_hot_arr  = np.array(one_hot_list)\n",
    "# one_hot_arr = torch.FloatTensor(one_hot_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0cdde28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f72e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import GPUtil\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import os\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "from performance import performances_val\n",
    "from PIL import Image\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.data import Data\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import ChebConv, GraphSAGE,GraphUNet ,TransformerConv\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "seed = 1378\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "landmark_colors = np.load(\"landmark_colors.npy\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_list=[]\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.expand_dims(landmark_colors,axis=1))  \n",
    "for x in landmark_colors:\n",
    "    one_hot_list.append(enc.transform([[x]]).toarray()[0])\n",
    "one_hot_arr  = np.array(one_hot_list)\n",
    "one_hot_arr = torch.FloatTensor(one_hot_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bc0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx_env",
   "language": "python",
   "name": "onnx_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
